---
title: "Data-Reanalysis-Assignment"
author: "Arora"
date: "December 3, 2017"
output: html_document
---
#Replication of Ellefsen and Smith 2016

##Introduction 
#####The following is the reanalysis of Ellefsen and Smith's 2016 paper Manuel hierarchical clustering of regional geochemical data using a Bayesian finite mixture model. 
#####In this study the authors use a Bayesian finite mixture model to cluster geochemical data from a USGS survey of Colorado. Their use of a Bayesian model is different from previous clustering techniques that others have previously used. 
#####In their paper they use hierarchical modeling to create two clusters of the data and within each of those clusters are two more clusters and so on, with each level of the cluster representing geochemical and geologic processes occuring at different spacial scales. 
#####To derieve the model parameters for their Bayesian model they use the Hamiltonian Monte Carlo sampling of the posterior probability function. However using this method of sampling gives several modes each with its own set of model paramters. The parameters are checked against previously known geologic knowledge and the model parameters which best fit the previous knowledge are used for the analysis. 
#####Spacial analysis of geochemical data allows for researchers to relate geologic, climatic, and biological processes with single or multiple element concentrations. For such as spacial analysis, clustering is a helpful technique especially since survey data can encompass thousands of samples with many elements measured (in my case I use at least 13 elements but in this study 44 elements were measured). These large datasets are difficult to analyze without multivariate techniques however geochemical data is compositional, rather than conventional. I will address what compositional data is and how to analyze this data below. 

##Dataset
#####The dataset used in the paper and in this replication can be found in the GcClust package which can be download from https://pubs.er.usgs.gov/publication/tm7C13
#####The dataset for this paper was collected for a USGS (United States Geological Service) survey of soil geochemistry in the state of Colorado. The original dataset for this study consisted of 966 samples with 44 elements measured. Six samples were excludeded because of issues with their location and one was removed because of anthropogenic effects. Five elements were removed because their measured concentrations often fell below their detection limits. This leaves 959 samples with 39 measured elements. Before analysis began the sum of the concentrations of the excluded elements were added up into a column, and the concentrations of the elements were scaled to mg/kg. 

##Outline of Analyses

#####Each of these steps, and the arguments for making them will be outline during the replication itself. 

#####1. Transform compositional data in isometric log-ratio (ilr) coordinates.
#####2. Transform ilr corrdinates with robust principle componetns transform. 
#####3. Select subset of components. 
#####4. Monte Carlo Sampling to produce the chains. 
#####5. Checking parameters within each chain produced by the Monte Carlo sampling. 
#####6. Selecting the chains for furthur analyses. 
#####7. Combining the chains and switching them into their proper place. 
#####8. Checking convergence of the chains. 
#####9. Calculate conditional probability that field sample is associated with the first probability density function in the finite mixture model.
#####10. Use the conditional probabilities to calculate mean vectors, standard deviations, and correlation matrices for both probability density functions for the finite mixture model. 
#####11. Plot the observed statistics (mean vectors, standard deviations, and correlation matrices). 
#####12. Transform the observed data into simplex, which is compositional center and variation matrix. 
#####13. Order the elements for visualization. 
#####14. Calculate the sample center, translate the compositional centers, and then plot these centers. 
#####15. Plot the combined variation matrix. 
#####16. Map the clusters. 

##Compositional Data



##Reanalysis
#####First download and open the following packages: {colorspace}, {GcClust}, {ggplot}, {maps}, {mvtnorm}, {reshape2}, {robustbase}, {rstan}, {sp}, {shiny}, {shinystan}
```{r}
library(colorspace)
library(GcClust)
library(ggplot2)
library(maps)
library(mvtnorm)
library(reshape2)
library(robustbase)
library(rstan)
library(sp)
library(shiny)
library(shinystan)
```
#####As per the instructions within the supplementary information for the paper, 
```{r}
gcData<-CoGeochemData 
#in their instructions they save their data at each point in .dat files however I did not.
#the data needs to be put into gcData because many of the functions (found on their github site here: https://github.com/USGS-R/GcClust/blob/master/R/GcClusterFunctions.R) require the dataset to be in a list called gcData. 
```

```{r}
maps::map(database="state", regions = "Colorado", fill=FALSE)
plot(gcData$concData, add = TRUE, pch =
16, cex = 1/3)
#their original code had fill, a white border, and red points however I could not get that particular code to work so I edited so the points were back and the fill was removed. 
```

![This is their map:](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Original_Map.jpg)

#####Now we are going to transform that data from compositional data to isometric log-ratio coordinates and then transformed again by the robust principle components transform. 
#####The first is done because normal statistics cannot be run on compositional data. The ilr transformation changes the data into a form that can be clustered. 
#####The second transformation is done, because the authors assume that transforming ilr coordinates, this will make the coming model more stable. This  transformation does reduce the number of dimensions and allows us to select with compoents will be useful for selecting which principle components are useful in describing variation within the data. 
```{r}
transData<-transformGcData(gcData)
#I decided not to use the code head(transData) because when doing so, it seems the entire data comes up rather than just the beginning
```
####The following are two plots of the principle components. The first is a box plot and the second is a violin plot, ,though they show the same information. As with box plots, the lower end of the whisper shows the 25th percentile and the end of the upper whisker shows the 75th percentile. At this stage my plots and their plots look exactly the same. 
```{r}
plotEdaDist(transData)
```

![These are their principle component plots:](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Principle_Component_Plots.jpg)


#####This is a correlation matrix of the principle components using (ADD MORE INFORMATION HERE ABOUT PEARSON CORRELATION) and below that graph is the histogram of the correlation components. 
```{r}
plotEdaCorr(transData)
```

![This is there correlation matrix and histogram for the principle components.](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Correlation_Matrix_for_PCA.jpg)


#####Now we will analyze a scree plot. In R in Action a Scree plot is described as:(ADD HERE). Using the major dip in the scree plot, we can determine the number of principle components. We want to choose enough principle components to describe as much of the data as possible however we want to exclude as much noise from the data as possible. And while choosing 39 principle components will describe all the data, we perform principle components analysis to essentially reduce the number of variables. 

```{r}
plotEdaVar(transData)
```
![This is the authors' scree plot.](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Scree_Plott.jpg)
#####The authors wanted to select enough principle components to explain 75% to 95% of the data. To do this we could choose any principle component starting at 6 and going until 21, however they decided to select 22 components because that represents 96% of the data. I would probably have chosen 10 principle components however this is the first time that I have used a scree plot and I have visualized PCA very differently using other programs. 

```{r}
nPCs <- 22
```

#####This next step is the addition of the Mixture Model from their github repository into the file. Since I have so little experience with Bayesian statistics and no knowledge of the stan programing language, I have yet to parse what their code is actually doing. However I will update this if I can figure it out. 
```{r}
tmp<-normalizePath(path.package("GcClust"))
load(paste(tmp, "\\stan\\MixtureModel.bin", sep=""))
```

#####The following code is used to sample the chains, however I had to edit this code because R did not seem to understand some of their code. This function is used in the samplePars function below. 
```{r}
sampleFmm <- function(transData, nPCs, sm,
                      priorParams,
                      nWuSamples = 500,
                      nPwuSamples = 500,
                      nChainsPerCore = 2,
                      nCores = 4,
                      procDir = ".") {


  rstanParallelSampler <- function(stanData, sm, nWuSamples, nPwuSamples,nChainsPerCore, nCores, procDir ) {

    CL <- parallel::makeCluster(nCores)

    parallel::clusterExport(cl = CL,
       c("stanData", "sm", "nWuSamples", "nPwuSamples","nChainsPerCore", "procDir"), envir=environment())

    fnlist <- parallel::parLapply(CL, 1:nCores, fun = function(cid) {
   # Make rstan available to the processors. This function won't work otherwise. 
      require(rstan, quietly = TRUE)

      fileNames <- vector(mode = "character", length = nChainsPerCore)

      for(i in 1:nChainsPerCore) {

        rng_seed <- sample.int(.Machine$integer.max,1)

        gen_inits <- function() {
          areInGrp1 <- sample(c(TRUE,FALSE), size = stanData$N,
                              prob = c(0.3, 0.7), replace = TRUE)
          return(list(
            theta = runif(1, min = 0.35, max = 0.65),
            mu1 = apply(stanData$Z[areInGrp1,], 2, mean ),
            mu2 = apply(stanData$Z[!areInGrp1,], 2, mean ),
            tau1 = apply(stanData$Z[areInGrp1,], 2, sd ),
            tau2 = apply(stanData$Z[!areInGrp1,], 2, sd ),
            L_Omega1 = diag(stanData$M),
            L_Omega2 = diag(stanData$M)
          ))
        }

        rawSamples <- rstan::sampling(sm, data = stanData,
                                      init = gen_inits,
                                      # control = list(stepsize = 0.00001),
                                      control = list(stepsize = 0.0001),
                                      # control = list(adapt_delta = 0.95),
                                      chains = 1,
                                      iter = nWuSamples + nPwuSamples,
                                      warmup = nWuSamples,
                                      seed = rng_seed, chain_id = cid,
                                      pars=c("theta", "mu1", "mu2",
                                             "tau1", "tau2",
                                             "L_Omega1", "L_Omega2", "log_lik"))#I took out save_dso=FALSE because R does not seem to understand what save_dso means

        fileNames[i] <- paste("RawSamples", cid, "-", i, ".dat", sep = "")
        save( rawSamples, file = paste(procDir, "\\", fileNames[i], sep = "") )

      }
      return(fileNames)
    } )

    parallel::stopCluster(CL)
    return(unlist(fnlist))
  }

  stanData <- list( M = nPCs,
                    N = nrow(transData$robustPCs),
                    Z = transData$robustPCs[,1:nPCs],
                    priorParams = priorParams )

  fileNames <- rstanParallelSampler(stanData, sm, nWuSamples, nPwuSamples,
                                    nChainsPerCore, nCores, procDir )

  return(list(nChains = nChainsPerCore * nCores,
              nWuSamples = nWuSamples,
              nPwuSamples = nPwuSamples,
              fileNames = fileNames))
}
```

#####Now its time to run the model. You will need to figure out how many cores you have and then select then number of chains you want run per core. They had more cores and ran more chains per core and got 35 chains in about 2 hours. I decided to run 20 chains and it took my computer over two hours to perform this. From this point on, their data and my data will look different. 
```{r}
priorParams <- c(4, 3, 3, 2)
samplePars <- sampleFmm(transData, nPCs,sm, priorParams, nChainsPerCore = 5, nCores = 4)
```

#####This is step 6, selecting which chains to use for furthur analyses. The problem we have is that the number of model parameters is very large; with 22 principle components the number of model parameters is 551, and that's just in one chain. To do this we will first plot the chains. 
```{r}
plotSelectedTraces(samplePars)
```

![#These are the plot traces for the first chain.](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Trace_Chain_1.jpg)

#####Now we will examine each mode (that is each chain) and select the mode which best fits the data. According to the authors it is temping to select the mode with the highest log-likelihood however the modes should be selected on how well they explain the geochemical data, not how high their log-likelihood ratio is. 
```{r}
plotPointStats(samplePars)
```
![These are the plots of the following vectors for the chains: mean, standard deviation, proportion, and log-likelihood. ](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Point_Statistics.jpg)


#####As you can see our plots are slightly different. The key to understanding the "switch" that they discuss in their supplementary data is to understand that the first probability density function is blue and the second probability density function is red. To complete the analysis they have outline below the blue chains should be on top and the red chains should be on the bottom. 

#####I too decided to choose the first four chains. Their explanation about how to choose modes that best fit the geochemical data was unclear; while I look for furthur resources about how to choose the best modes for the given data I decided to follow their example. 


#####Now set up a csv file with the chains you have selected. As stated above I decided to select the first four chains however X has the two chains switched. If I was following their example I would switch the chains so that all of the first probability density function information is "on top" of the second probability density function. However when attempting to perform the switch, the if then statement does not work. I have worked through the function and have yet to come up with a solution and so have replaced combineChains with combineChainz. 
```{r}
f<-read.csv(file="C:/Users/Arora/Desktop/selectedChains.csv")
f    
```
```{r}
selectedChains<-read.csv(file="C:/Users/Arora/Desktop/selectedChains.csv", header=TRUE, stringsAsFactors = FALSE)
```


```{r}
combinedChains <- combineChains(samplePars,selectedChains)
```

```{r}
combineChainz <- function(samplePars, selectedChains, procDir = ".") {
  sfList <- vector(mode = "list")
  for(k in 1:nrow(selectedChains)) {
    iChain <- selectedChains[k, "Chain"]
    load( paste(procDir, "\\", samplePars$fileNames[iChain], sep = ""))
    sfList[[k]] <- rawSamples
  return(rstan::sflist2stanfit(sfList))
  }
}
#I could not get the if statement to work, so while I figure that out I decided to create this new function combinedChaniz (with a z) and remove to if statement. This means that the first and fourth chains (which have been switched around) will not be in the correct order which will of course, change the analysis
```
#####This step combines the chains together so that we can use them for the next step. 
```{r}
combinedChains<-combineChainz(samplePars, selectedChains, procDir = ".")
```
#####Now its time to check the model, which includes the mean vectors, the standard deviation vectors, and the correlation matrices for the two probability density functions. To check the model these are compared with the same statistics calculated from the principle components. 

#####To begin checking the model, we need to calculate the conditional probability that a particular sample is associated with the first probability density function. These conditional probabilities, along with the principle components are used to calculate the mean vectors, standard deviation vectors, and correlation matrices necessary for checking our model. 
```{r}
condProbs1 <- calcCondProbs1 (transData,nPCs, combinedChains)
```
#####This matrix is calculated from the 22 principle components and the conditional probabilities. 
```{r}
obsTestStats <- calcObsTestStats (transData,nPCs, condProbs1)
```
#####To check the model we will compare the observed statistics (code right above this) with the replicated test statistics which we get from samples of the posterior probability density function. 
#####In this study, the replicated test statistics are the same as the samples of the mean vectors, standard deviation vectors, and correlation coefficients that we combined together in CombinedChains.
#####The following is a plot of the mean and standard deviation vectors. The red dot is the test statistic which should be close to the median, the vertical black line represents the 95% confidence interval. This figure also gives p value between the observed and replicated statistics. 
```{r}
plotTMeanSd(combinedChains, obsTestStats)
#neither of these plots look particularly off from the ones in the paper
#explain my results here
```

![These are the plots of the vectors for both probabilityh density functions fo model checking. ](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Model_Checking.jpg)

#####Now its time to compare the correlation matrices between the test statistics and the replicated statistics. 
```{r}
plotTCorr(combinedChains, obsTestStats)
#this set of four graphs looks very complex at first. Let's first concentrate on the first and third graphs which are fully coloured and have the red diagonal line. The top right triangle of each graph represents the correlation matrix from the principle components. The bottom left triangle represents the correlation matrix from the replicated samples. The comparison between the two is do they look the same? In their case their graphs do look the same across the red line. 
#The two blue graphs which only fill the top right portion of the graph present the p-values. However these values will be mirror across the diagonal line so both sides are not shown. In this particular analysis, the largest possible posterior predictive p-value is 0.5 and so in their graphs the p-values are, as they state, moderate to large. 
```

![These are the plots of the correlation matrices for both probabilityh density functions fo model checking. ](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Model_Check_Matrices.jpg)


#####The values from combinedChains (mean vector, standard deviation vector, and correlation matrices) are transformed in simplex. I continue to investigate why this transformation needs to take place and what happens. 
```{r}
simplexModPar <- backTransform (gcData,nPCs, transData, combinedChains)
```
#####This is reording the elements. I would have done this alphabetically or perhaps in the order of major elements followed by trace elements. 
```{r}
elementOrder <- c("Sr", "U", "Y", "Nb", "La", "Ce", "Th", "Na", "Al", "Ga","Be", "K", "Rb", "Ba", "Pb", "Cu", "Zn", "Mo", "Mg", "Sc", "Co", "Fe", "V", "Ni","Cr", "Ca", "P", "Ti", "Li", "Mn", "Sn","As", "Bi", "Cd", "In", "S", "Sb", "Tl","W", "EE")
```

#####Using the elementOrder decided on above, median of the compositional centers for each element (the compositional centers is a vector) is plotted. This graph is useful because the units are in concentration, making it easy to compare to other datasets.
#####In the graph there are two dots: one red and one blue. For a particular element the blue dot represents the median concentration as calculated by the first probability density function, and the red dot presents the median concentration as calculated by the second probability density function. 

```{r}
plotCompMeans(simplexModPar, elementOrder)
```
![This is the plot of the compositional centers from Ellefsen and Smith:](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Compositional_Centers.jpg)

#####Using simplex again, the statistics are translated to allow us to tell the difference between the medians of the first probability density function versus the second. Ellefsen and Smith also want to add additional information to the plot involving the replicated statistics. 
```{r}
simplexStats <- calcSimplexStats(gcData)
```

#####This plot has thhe translated data which is cleared to read and has the addition of the 95% confidence interval. As always, blue represents the first probaility density function and red represents the second. The only problem is is that with the transformation, we loose the units of concentration and so this data is less comparable to other data sets. 
```{r}
plotTransCompMeans( simplexModPar,simplexStats, gcData, elementOrder)
```
![This is the plot of the translated compositional centers:](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Translated_Compositional_Centers.jpg)

#####Our next visualization is looking at the variation matrices for both probability density functions. Like the second and fourth graphs produced in the plotTCorr, the graph is symmetrical across the diagonal line. Unlike the plots in plotTCorr, Ellefsen and Smith decided to combine both variation matrices into one matrix. 
#####The first probability density function is in the upper right triangle and the second is in the lower left triangle, which I think makes understanding the graphs more difficult. I'll be looking through their code and trying to find a way to split the two matrices. 
#####This graph is the the standard deviation of the log-ratios between two elements. I've done similar analyses before however each element is plotted against each other individually which makes the graphs very large but its easier to see the variation (in the case I'm discussing a lot of variation looks like a cloud with no patterns; I'll try and insert an image of it in here however the computer we use might be in Greece still).
#####In mine.....In theirs, it looks like the variance is smaller in the second probability density function than in the first. 
```{r}
plotSqrtVarMatrices( simplexModPar,elementOrder, colorScale = "rainbow" )
```

![This is the authors' plot of the variation correlation matrices:](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Variation_Matrix.jpg)


#####Let's graph the clusters onto the map of Colorado. There are four categories created.
#####The association between the field sample and the probability distribution is calculated using the range of median probabilities calculated within the conditional probability matrix for the first probability density function (condProb1). The key to interpreting the map and the median probabilities associated with each category explained in the table below. 
```{r}
m<-read.csv(file="C:/Users/Arora/Desktop/MapKey.csv")
m    
map(database = "state", regions = "colorado", fill = TRUE, col = "grey95", border = "white")
map.axes()
plotClusters(gcData, condProbs1, symbolSizes = rep.int(2/3, 4))
```
#####Glancing at mine.....and from their, most of the points look either strongly associated with the first probability density function or moderately associated with the second. 
![This is their map of mode one:](https://github.com/geochemica/data-reanalysis-assignment/blob/master/Map_Mode.jpg)


